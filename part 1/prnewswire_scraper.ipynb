{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prnewswire_scraper.ipynb",
      "provenance": [],
      "mount_file_id": "1Zj2ahgWyGlzoihMMYG2PR7HXd1JIMRAO",
      "authorship_tag": "ABX9TyPx0rnLRANsvg7YnN3whrS3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zantorym/AIDI-1100-Project/blob/main/prnewswire_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "scS2G_N54g3y",
        "outputId": "bee92dd5-a913-454b-8075-66094e6a9cd2"
      },
      "source": [
        "\"\"\"PRNewswire article scraper\n",
        "\n",
        "This script scrapes all the PRNewswire articles released within a specified \n",
        "number of days from the current date and time.\n",
        "\n",
        "This script can be imported as a module and contains the following functions:\n",
        "    * _get_urls - returns the urls for all the articles released up till a \n",
        "                  certain date\n",
        "    * _get_articles - returns the text of the body of the articles corresponding\n",
        "                      to each url\n",
        "    * scrape - the main function of the script that runs everything\n",
        "\n",
        "Functions that start with an underscore are 'private' and are not meant to be \n",
        "called when this module is imported. These functions will not be copied over \n",
        "when this module is imported using the line 'from prnewswire_scraper import *'.\n",
        "However, they will be copied over if the module is imported using the line\n",
        "'import prnewswire_scraper' but this is unavoidable as there is no real way to\n",
        "maintain 'private' functions of modules in python.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"PRNewswire article scraper\\n\\nThis script scrapes all the PRNewswire articles released within a specified \\nnumber of days from the current date and time.\\n\\nThis script can be imported as a module and contains the following functions:\\n    * _get_urls - returns the urls for all the articles released up till a \\n                  certain date\\n    * _get_articles - returns the text of the body of the articles corresponding\\n                      to each url\\n    * scrape - the main function of the script that runs everything\\n\\nFunctions that start with an underscore are 'private' and are not meant to be \\ncalled when this module is imported. These functions will not be copied over \\nwhen this module is imported using the line 'from prnewswire_scraper import *'.\\nHowever, they will be copied over if the module is imported using the line\\n'import prnewswire_scraper' but this is unavoidable as there is no real way to\\nmaintain 'private' functions of modules in python.\\n\\n\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFzEOVL1I7_Z"
      },
      "source": [
        "# Importing modules\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pytz import timezone\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-qgW_ZEJlcL"
      },
      "source": [
        "# Gets urls for all the articles from start date to end date\n",
        "# Returns a list of urls\n",
        "def _get_urls(end_date):\n",
        "  urls = [] # List of URLs to visit\n",
        "\n",
        "  website = \"https://www.prnewswire.com/news-releases/news-releases-list/?page=\" # Website we need to scrape from\n",
        "  page_num = 1 # Page number of the website we need to scrape from\n",
        "\n",
        "  end_date_reached = False\n",
        "\n",
        "  while not end_date_reached:\n",
        "    current_site = website + str(page_num) + \"&pagesize=100\" # Link to visit with page number, set number of articles per page to 100 so that we don't need to visit as many pages\n",
        "    response = requests.get(current_site)\n",
        "\n",
        "    if response.status_code == 200: # 200 is the standard response for a successful HTTP request\n",
        "      soup = BeautifulSoup(response.content) # Converting the plain text html code of the website into a BeautifulSoup object for easy parsing\n",
        "      anchors = soup.find_all('a', {'class': 'newsreleaseconsolidatelink display-outline', 'href': True}) # Getting all the anchors for news articles within the webpage\n",
        "\n",
        "      for anchor in anchors:\n",
        "        date = anchor.find('small').get_text()\n",
        "        try: \n",
        "          date = datetime.strptime(date, '%b %d, %Y, %H:%M ET') # Convert to datetime\n",
        "        except: # If the conversion fails, that is because the article was releasaed today and the time is written as \"HH:MM ET\" instead of \"Month DD, YYYY, HH:MM ET\"\n",
        "          date = datetime.strptime(date, '%H:%M ET') # Convert the time into a datetime variable\n",
        "          now = datetime.now(timezone('EST')).date() # Get today's date, had to add timezone because google colab operates on UTC, while prnewswire operates on EST\n",
        "          now_t = datetime.time(date) # Time the article was released\n",
        "          date = datetime.combine(now, now_t) # Date and time combined\n",
        "        \n",
        "        if (date < end_date):\n",
        "          end_date_reached = True\n",
        "          break\n",
        "        else:\n",
        "          href = \"https://www.prnewswire.com\" + anchor['href'] # Retrieving href for the article and converting it to visitable link\n",
        "          urls.append(href) # Adding to list of urls to visit\n",
        "      \n",
        "      page_num += 1\n",
        "    \n",
        "\n",
        "\n",
        "  return urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfnXCuXqPAF5"
      },
      "source": [
        "def _get_articles(urls):\n",
        "  articles = []\n",
        "\n",
        "  for url in urls:\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.content)\n",
        "\n",
        "      divs = soup.find_all('div', class_ = 'col-sm-10 col-sm-offset-1') # Finds all div containers of the class that's meant for the body of the webpage\n",
        "\n",
        "      # Getting all the text out of the divs collected above\n",
        "      article = [] # For storing all the text within this article\n",
        "      for div in divs:\n",
        "        p_tags = div.find_all('p') # All the p tags in the current div, since all the text in the body of the prnewsire articles is always stored within p tags\n",
        "        for p in p_tags:\n",
        "          article.append(p.get_text())\n",
        "      articles.append(article)\n",
        "\n",
        "  return articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1QL7Hrh8kry"
      },
      "source": [
        "def scrape(NUM_DAYS):\n",
        "  \n",
        "  end_date = datetime.today() - timedelta(days=NUM_DAYS) # Date NUM_DAYS days ago\n",
        "  urls = _get_urls(end_date)\n",
        "  articles = _get_articles(urls)\n",
        "\n",
        "  return articles"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
