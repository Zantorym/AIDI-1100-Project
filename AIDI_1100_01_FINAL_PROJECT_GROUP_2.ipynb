{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIDI_1100_01_FINAL-PROJECT_GROUP-2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqvAiHqHR/fz4Gsw+Hp3Sn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zantorym/AIDI-1100-Project/blob/main/AIDI_1100_01_FINAL_PROJECT_GROUP_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cCHrgMgltN2"
      },
      "source": [
        "# AIDI1100 Final Project\n",
        "\n",
        "\n",
        "---\n",
        "**Group Number:** 2\n",
        "\n",
        "**Group Members:**\n",
        "\n",
        "Jaspreet Singh Marwah\n",
        "\n",
        "Lawrence Wanderi Mwangi\n",
        "\n",
        "Sherap Gyaltsen\n",
        "\n",
        "Ayobami Banjoko\n",
        "\n",
        "Oluwaseun Ogunnubi\n",
        "\n",
        "Simrandeep Singh Rahi\n",
        "\n",
        "**Course:** AIDI1100 - Introduction To AI Development\n",
        "\n",
        "**Submission Date:** \n",
        "\n",
        "---\n",
        "\n",
        "**Strategy of Code Distribution:**\n",
        "\n",
        "Part 1 (Scan/Parse): Jaspreet\n",
        "\n",
        "Part 2 (Track/Store): Lawrence Wanderi Mwangi\n",
        "\n",
        "Part 3 (Retrieve Data): Sherap Gyaltsen\n",
        "\n",
        "Part 4 (Visualize): Ayobami Banjoko, Oluwaseun Ogunnubi, Simrandeep Singh Rahi\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Program Description:**\n",
        "\n",
        "*Part 1:* \n",
        "\n",
        "First, using the _get_urls() function, the program goes through the list of articles found on https://www.prnewswire.com/news-releases/news-releases-list. It gets the raw html code of the website. Using the BeautifulSoup module, the program parses through the html code and collects the URLs for all the articles released up til a certain date and time. If necessary, the program will keep going to the next page of the website until the URLs of all the articles released till the required date have been obtained.\n",
        "\n",
        "Then, using _get_articles(), for each URL obtained (where each URL corresponds to one article), the raw HTML code is extracted and parsed through using BeautifulSoup. The program finds all the text from the body of the articles and stores it in a list.\n",
        "\n",
        "The scrape function is used to conveniently execute both functions based on a specified number of days worth of articles. It then returns a list of all the text from the articles. This function allows this part of the code to act as an individual module. More information regarding this can be found in the \"Bonus Work\" section of this file.\n",
        "\n",
        "*Part 2:*\n",
        "\n",
        "All the text from the articles is merged into one large string. This string is parsed over using a regex pattern to find all stock symbols within the articles collected.\n",
        "\n",
        "*Part 3:*\n",
        "\n",
        "3 of the stock symbols that were collected are chosen for analysis, namely TSLA, GM and LCID. Given a start date, end date, and stock symbol, the stock_data() function will provide information regarding the closing price and volume of the specified stock for each day within the range of dates provided. This function does this by calling the Yahoo Finance API with the help of the yahoo_fin package.\n",
        "\n",
        "*Part 4:*\n",
        "\n",
        "generate_visualisations() takes advantage of the plotly and matplotlib libraries to generate interactive time-series plots of the volume and closing price of a given stock within a 60 day period. When the function is called with a stock name, it displays the two plots.\n",
        "\n",
        "---\n",
        "\n",
        "**Bonus work:**\n",
        "\n",
        "\n",
        "\n",
        "*   GitHub was used to colaborate on the project. GitHub link: https://github.com/Zantorym/AIDI-1100-Project\n",
        "*   The code for part 1 can be used as a module. When imported, the scrape() function will scrape all the latest PRNewswire articles published a specified number of days ago. This module is available on the GitHub repository as \"prnewswire_scraper.ipynb\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMAWVVAeh3lj"
      },
      "source": [
        "# PART 1 - Scan/Parse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IrqzaUqhsLy"
      },
      "source": [
        "\"\"\"PRNewswire article scraper\n",
        "\n",
        "This script scrapes all the PRNewswire articles released within a specified \n",
        "number of days from the current date and time.\n",
        "\n",
        "This script can be imported as a module and contains the following functions:\n",
        "    * _get_urls - returns the urls for all the articles released up till a \n",
        "                  certain date\n",
        "    * _get_articles - returns the text of the body of the articles corresponding\n",
        "                      to each url\n",
        "    * scrape - the main function of the script that runs everything\n",
        "\n",
        "Functions that start with an underscore are 'private' and are not meant to be \n",
        "called when this module is imported. These functions will not be copied over \n",
        "when this module is imported using the line 'from prnewswire_scraper import *'.\n",
        "However, they will be copied over if the module is imported using the line\n",
        "'import prnewswire_scraper' but this is unavoidable as there is no real way to\n",
        "maintain 'private' functions of modules in python.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Importing modules\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pytz import timezone\n",
        "import re\n",
        "\n",
        "pip install yahoo_fin\n",
        "from yahoo_fin.stock_info import get_data\n",
        "\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw1_oHphjEr_"
      },
      "source": [
        "# Gets urls for all the articles from start date to end date\n",
        "# Returns a list of urls\n",
        "def _get_urls(end_date):\n",
        "  urls = [] # List of URLs to visit\n",
        "\n",
        "  website = \"https://www.prnewswire.com/news-releases/news-releases-list/?page=\" # Website we need to scrape from\n",
        "  page_num = 1 # Page number of the website we need to scrape from\n",
        "\n",
        "  end_date_reached = False\n",
        "\n",
        "  while not end_date_reached:\n",
        "    current_site = website + str(page_num) + \"&pagesize=100\" # Link to visit with page number, set number of articles per page to 100 so that we don't need to visit as many pages\n",
        "    response = requests.get(current_site)\n",
        "\n",
        "    if response.status_code == 200: # 200 is the standard response for a successful HTTP request\n",
        "      soup = BeautifulSoup(response.content) # Converting the plain text html code of the website into a BeautifulSoup object for easy parsing\n",
        "      anchors = soup.find_all('a', {'class': 'newsreleaseconsolidatelink display-outline', 'href': True}) # Getting all the anchors for news articles within the webpage\n",
        "\n",
        "      for anchor in anchors:\n",
        "        date = anchor.find('small').get_text()\n",
        "        try: \n",
        "          date = datetime.strptime(date, '%b %d, %Y, %H:%M ET') # Convert to datetime\n",
        "        except: # If the conversion fails, that is because the article was releasaed today and the time is written as \"HH:MM ET\" instead of \"Month DD, YYYY, HH:MM ET\"\n",
        "          date = datetime.strptime(date, '%H:%M ET') # Convert the time into a datetime variable\n",
        "          now = datetime.now(timezone('EST')).date() # Get today's date, had to add timezone because google colab operates on UTC, while prnewswire operates on EST\n",
        "          now_t = datetime.time(date) # Time the article was released\n",
        "          date = datetime.combine(now, now_t) # Date and time combined\n",
        "        \n",
        "        if (date < end_date):\n",
        "          end_date_reached = True\n",
        "          break\n",
        "        else:\n",
        "          href = \"https://www.prnewswire.com\" + anchor['href'] # Retrieving href for the article and converting it to visitable link\n",
        "          urls.append(href) # Adding to list of urls to visit\n",
        "      \n",
        "      page_num += 1\n",
        "    \n",
        "\n",
        "\n",
        "  return urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8910TR4jGPL"
      },
      "source": [
        "def _get_articles(urls):\n",
        "  articles = []\n",
        "\n",
        "  for url in urls:\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.content)\n",
        "\n",
        "      divs = soup.find_all('div', class_ = 'col-sm-10 col-sm-offset-1') # Finds all div containers of the class that's meant for the body of the webpage\n",
        "\n",
        "      # Getting all the text out of the divs collected above\n",
        "      article = [] # For storing all the text within this article\n",
        "      for div in divs:\n",
        "        p_tags = div.find_all('p') # All the p tags in the current div, since all the text in the body of the prnewsire articles is always stored within p tags\n",
        "        for p in p_tags:\n",
        "          article.append(p.get_text())\n",
        "      articles.append(article)\n",
        "\n",
        "  return articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF4hy-nyjJJP"
      },
      "source": [
        "def scrape(NUM_DAYS):\n",
        "  \n",
        "  end_date = datetime.today() - timedelta(days=NUM_DAYS) # Date NUM_DAYS days ago\n",
        "  urls = _get_urls(end_date)\n",
        "  articles = _get_articles(urls)\n",
        "\n",
        "  return articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWrAK7oIjb7R"
      },
      "source": [
        "dataset = scrape(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HkhJBIhjTBH"
      },
      "source": [
        "## PART 2 - Track/Store/Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1Cw3odojXMe"
      },
      "source": [
        "corpus = ''\n",
        "for x in dataset:\n",
        "  corpus += ' '.join(str(e) for e in x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_lTWgbbju4X"
      },
      "source": [
        "def tickerCapture(corpus:str) -> list:\n",
        "    \"\"\"Returns a list of tuples that have this format (<Exchange>, <ticker>)\n",
        "\n",
        "    Args:\n",
        "        corpus (str): String that contains exchang and ticker info\n",
        "\n",
        "    Returns:\n",
        "        list: [(<Exchange>, <ticker>)] list of <exchange> <ticker> data\n",
        "    \"\"\"\n",
        "     # Expected formats examples: (NYSE: HMLP) and  (NYSE/LSE: CCL; NYSE:CUK)\n",
        "     # Example return [('NYSE', 'HMLP'), ('NYSE/LSE', 'CCL'), ('NYSE', 'CUK')]\n",
        "\n",
        "    regexpattern = r'\\b\\(?(?P<exchange>[A-Z\\/]+):\\s?(?P<ticker>[A-Z]+)(?:\\)|;)'\n",
        "\n",
        "    result = re.findall(regexpattern, corpus)\n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHEiTywzjxNC"
      },
      "source": [
        "tickers = tickerCapture(corpus)\n",
        "tickers = list(set(tickers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAGjWeOej7TQ"
      },
      "source": [
        "# PART 3 - Retrieve Data (Web (API))\n",
        "\n",
        "From the list of tickers that we retrieved, we will select the following 3 that belong to the automobile industry: TSLA, GM, and LCID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrTxqVrSkDa9"
      },
      "source": [
        "def stock_data(start_date,end_date,ticker):\n",
        "  k = get_data(ticker, start_date=start_date, end_date=end_date, index_as_date = True, interval='1d')\n",
        "  resultant = k[['close','volume']]\n",
        "  return resultant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akm5z9JVlQBX"
      },
      "source": [
        "# PART 4 - Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6KO9xvOlUDc"
      },
      "source": [
        "def generate_visualisations(ticker):\n",
        "  # Getting info regarding the stock from the past 60 days using the ticker\n",
        "  stock_info = stock_data('09/23/2021', '11/23/2021', ticker)\n",
        "  stock_info = stock_info.rename_axis('date').reset_index()\n",
        "\n",
        "  # Plotting time-series for volume of stock\n",
        "  fig = px.line(stock_info, x='date', y=\"volume\", title= f\"Volume of {ticker} stock\")\n",
        "  fig.update_xaxes(\n",
        "      rangeslider_visible=True,\n",
        "      rangeselector=dict(\n",
        "          buttons=list([\n",
        "              dict(count=7, label=\"7d\", step=\"day\", stepmode=\"backward\"),\n",
        "              dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
        "              dict(step=\"all\")\n",
        "          ])\n",
        "      )\n",
        "  )\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "  # Generating time-series for closing price of stock\n",
        "  fig = px.line(stock_info, x='date', y=\"close\", title= f\"Closing price of {ticker} stock\")\n",
        "  fig.update_xaxes(\n",
        "      rangeslider_visible=True,\n",
        "      rangeselector=dict(\n",
        "          buttons=list([\n",
        "              dict(count=7, label=\"7d\", step=\"day\", stepmode=\"backward\"),\n",
        "              dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
        "              dict(step=\"all\")\n",
        "          ])\n",
        "      )\n",
        "  )\n",
        "\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D05INqEflZZC"
      },
      "source": [
        "generate_visualisations('TSLA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGoSgZdGla5s"
      },
      "source": [
        "generate_visualisations('GM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ0RvWLSlbeO"
      },
      "source": [
        "generate_visualisations('LCID')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}