{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIDI_1100_01_FINAL-PROJECT_GROUP-2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zantorym/AIDI-1100-Project/blob/main/AIDI_1100_01_FINAL_PROJECT_GROUP_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cCHrgMgltN2"
      },
      "source": [
        "# AIDI1100 Final Project\n",
        "\n",
        "\n",
        "---\n",
        "**Group Number:** 2\n",
        "\n",
        "**Group Members:**\n",
        "\n",
        "Jaspreet Singh Marwah\n",
        "\n",
        "Lawrence Wanderi Mwangi\n",
        "\n",
        "Sherap Gyaltsen\n",
        "\n",
        "Ayobami Banjoko\n",
        "\n",
        "Oluwaseun Ogunnubi\n",
        "\n",
        "Simrandeep Singh Rahi\n",
        "\n",
        "**Course:** AIDI1100 - Introduction To AI Development\n",
        "\n",
        "**Submission Date:** 30th October, 2021\n",
        "\n",
        "---\n",
        "\n",
        "**Strategy of Code Distribution:**\n",
        "\n",
        "Part 1 (Scan/Parse): Jaspreet\n",
        "\n",
        "Part 2 (Track/Store): Lawrence Wanderi Mwangi\n",
        "\n",
        "Part 3 (Retrieve Data): Sherap Gyaltsen\n",
        "\n",
        "Part 4 (Visualize): Ayobami Banjoko, Oluwaseun Ogunnubi, Simrandeep Singh Rahi\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Program Description:**\n",
        "\n",
        "*Part 1 (Scan/Parse):* \n",
        "\n",
        "First, using the _get_urls() function, the program goes through the list of articles found on https://www.prnewswire.com/news-releases/news-releases-list. It gets the raw html code of the website. Using the BeautifulSoup module, the program parses through the html code and collects the URLs for all the articles released up til a certain date and time. If necessary, the program will keep going to the next page of the website until the URLs of all the articles released till the required date have been obtained.\n",
        "\n",
        "Then, using _get_articles(), for each URL obtained (where each URL corresponds to one article), the raw HTML code is extracted and parsed through using BeautifulSoup. The program finds all the text from the body of the articles and stores it in a list.\n",
        "\n",
        "The scrape function is used to conveniently execute both functions based on a specified number of days worth of articles. It then returns a list of all the text from the articles. This function allows this part of the code to act as an individual module. More information regarding this can be found in the \"Bonus Work\" section of this file.\n",
        "\n",
        "*Part 2 (Track/Store):*\n",
        "\n",
        "All the text from the articles is merged into one large string. This string is parsed over using a regex pattern to find all stock symbols within the articles collected.\n",
        "\n",
        "*Part 3 (Retrieve Data):*\n",
        "\n",
        "3 of the stock symbols that were collected are chosen for analysis, namely TSLA, GM and LCID. Given a start date, end date, and stock symbol, the stock_data() function will provide information regarding the closing price and volume of the specified stock for each day within the range of dates provided. This function does this by calling the Yahoo Finance API with the help of the yahoo_fin package.\n",
        "\n",
        "*Part 4 (Visualize):*\n",
        "\n",
        "generate_visualisations() takes advantage of the plotly and matplotlib libraries to generate interactive time-series plots of the volume and closing price of a given stock within a 60 day period. When the function is called with a stock name, it displays the two plots.\n",
        "\n",
        "---\n",
        "\n",
        "**Bonus work:**\n",
        "\n",
        "\n",
        "\n",
        "*   GitHub was used to colaborate on the project. GitHub link: https://github.com/Zantorym/AIDI-1100-Project\n",
        "*   The code for part 1 can be used as a module. When imported, the scrape() function will scrape all the latest PRNewswire articles published a specified number of days ago. This module is available on the GitHub repository as \"prnewswire_scraper.ipynb\".\n",
        "*   The dataset obtained from part 1 (the text from all the articles) and part 2 (the list of stock symbols) was pickled and uploaded to our GitHub repository for convenience as well as to ensure that all group members were working on the same dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMAWVVAeh3lj"
      },
      "source": [
        "# PART 1 - Scan/Parse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IrqzaUqhsLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21b6ff7f-57c3-4509-f0e4-2a4fe20d7fff"
      },
      "source": [
        "\"\"\"Scraping PRNewswire articles\n",
        "\n",
        "This part of the program scrapes all the PRNewswire articles released within a \n",
        "specified number of days from the current date and time.\n",
        "\n",
        "This part of the program is available on our GitHub repository as an individual \n",
        "script that can be imported as a module and contains the following functions:\n",
        "    * _get_urls - returns the urls for all the articles released up till a \n",
        "                  certain date\n",
        "    * _get_articles - returns the text of the body of the articles corresponding\n",
        "                      to each url\n",
        "    * scrape - the main function of the script that runs everything\n",
        "\n",
        "Functions that start with an underscore are 'private' and are not meant to be \n",
        "called when the module version of this code is imported. These functions will \n",
        "not be copied over when the module is imported using the line 'from prnewswire_scraper import *'.\n",
        "However, they will be copied over if the module is imported using the line\n",
        "'import prnewswire_scraper' but this is unavoidable as there is no real way to\n",
        "maintain 'private' functions of modules in python.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Importing modules\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pytz import timezone\n",
        "import re\n",
        "\n",
        "!pip install yahoo_fin\n",
        "from yahoo_fin.stock_info import get_data\n",
        "\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yahoo_fin\n",
            "  Downloading yahoo_fin-0.8.9.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from yahoo_fin) (2.23.0)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from yahoo_fin) (1.1.5)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->yahoo_fin) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->yahoo_fin) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->yahoo_fin) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->yahoo_fin) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (1.24.3)\n",
            "Collecting w3lib\n",
            "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting pyppeteer>=0.0.14\n",
            "  Downloading pyppeteer-0.2.6-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting parse\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "Collecting pyquery\n",
            "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from requests-html->yahoo_fin) (0.0.1)\n",
            "Collecting websockets<10.0,>=9.1\n",
            "  Downloading websockets-9.1-cp37-cp37m-manylinux2010_x86_64.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 67.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.8.2)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (1.4.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 55.5 MB/s \n",
            "\u001b[?25hCollecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html->yahoo_fin) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html->yahoo_fin) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->requests-html->yahoo_fin) (4.6.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.7/dist-packages (from pyquery->requests-html->yahoo_fin) (4.2.6)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: fake-useragent, parse, sgmllib3k\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=a9aed1472e46d3a777f7793d93d3ab8ad2a2744ad74d265160e881c6343c1194\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=d72fedc71dc31673a4296b579bb4f19eff8753adb75ebef63a6413ab47854ff8\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/aa/cc/f2228050ccb40f22144b073f15a2c84f11204f29fc0dce028e\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=d4f128a3d474ea2ab32f9ec6884e196c408af3802bfdfee6b2969e50db810e28\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built fake-useragent parse sgmllib3k\n",
            "Installing collected packages: websockets, urllib3, pyee, cssselect, w3lib, sgmllib3k, pyquery, pyppeteer, parse, fake-useragent, requests-html, feedparser, yahoo-fin\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed cssselect-1.1.0 fake-useragent-0.1.11 feedparser-6.0.8 parse-1.19.0 pyee-8.2.2 pyppeteer-0.2.6 pyquery-1.4.3 requests-html-0.10.0 sgmllib3k-1.0.0 urllib3-1.25.11 w3lib-1.22.0 websockets-9.1 yahoo-fin-0.8.9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw1_oHphjEr_"
      },
      "source": [
        "def _get_urls(end_date):\n",
        "  \"\"\"\n",
        "  Gets urls for all the articles released on PRNewswire from the time this\n",
        "  function is called till a specified end date.\n",
        "\n",
        "  Args:\n",
        "      end_date (datetime object): The oldest allowed date and time. If an article \n",
        "                                  was published before this date and time, it \n",
        "                                  will not be scraped.\n",
        "\n",
        "  Returns:\n",
        "      urls (list): A list of URLs corresponding to each article released within\n",
        "                  the specified time period\n",
        "\n",
        "  Detailed description:\n",
        "  We start by visiting the first page of the news release list on PRNewswire.\n",
        "  When we visit the page, in order to avoid errors, we confirm if the visit was \n",
        "  successful by checking if we got a response status code of 200 (what is \n",
        "  conventionally the standard response for a successful HTTP request). If the\n",
        "  response status code is not 200, the program will keep trying to visit the\n",
        "  website until it succeeds. \n",
        "  \n",
        "  Once a successful visit has been established, using BeautifulSoup we parse \n",
        "  through the raw HTML code for the website. When looking at the raw html code\n",
        "  manually, a pattern was observed. Each entry for an article is contained within\n",
        "  an anchor tag of the class 'newsreleaseconsolidatelink display-outline'. So,\n",
        "  out first step is to get all these anchor tags and store them in a list.\n",
        "\n",
        "  Then, for each anchor tag, we find the date and time that article was released.\n",
        "  Within each anchor tag, the date and time is enclosed within a 'small' tag.\n",
        "  Moreover, it is always the only thing enclosed within a 'small' tag. So, to\n",
        "  get the date and time of the release of the article, we extract the text\n",
        "  enclosed within the 'small' tag. We then convert it into a date. The date\n",
        "  itself can be represented in two ways, depending on when the article was\n",
        "  realeased. If the article was released today, it would only have the hour and\n",
        "  minute it was released in a 24-hour format. If it was released any other day,\n",
        "  it would have the full date and time. The code accounts for both possibilities\n",
        "  via a try-except statement. The date is then converted into a datetime variable.\n",
        "  If the date is older than our end date, we know that we don't need to explore\n",
        "  any more articles and we stop the process. Otherwise, we get the link to the\n",
        "  article and store it in a list. If all the articles from the page have been\n",
        "  covered and we still have not reached our end date, we move on to the next\n",
        "  page.\n",
        "\n",
        "  \"\"\"\n",
        "  urls = [] # List of URLs to visit\n",
        "\n",
        "  website = \"https://www.prnewswire.com/news-releases/news-releases-list/?page=\" # Website we need to scrape from\n",
        "  page_num = 1 # Page number of the website we need to scrape from\n",
        "\n",
        "  end_date_reached = False # A boolean to keep track of whether we've collected all the necessary articles\n",
        "\n",
        "  while not end_date_reached:\n",
        "    current_site = website + str(page_num) + \"&pagesize=100\" # Link to visit with page number, set number of articles per page to 100 so that we don't need to visit as many pages\n",
        "    response = requests.get(current_site)\n",
        "\n",
        "    if response.status_code == 200: # 200 is the standard response for a successful HTTP request. This condition ensures that we were successful in retrieving the HTML code for the website.\n",
        "      soup = BeautifulSoup(response.content) # Converting the plain text html code of the website into a BeautifulSoup object for easy parsing\n",
        "      anchors = soup.find_all('a', {'class': 'newsreleaseconsolidatelink display-outline', 'href': True}) # Getting all the anchors for news articles within the webpage\n",
        "\n",
        "      for anchor in anchors:\n",
        "        date = anchor.find('small').get_text()\n",
        "        try: \n",
        "          date = datetime.strptime(date, '%b %d, %Y, %H:%M ET') # Convert to datetime\n",
        "        except: # If the conversion fails, that is because the article was releasaed today and the time is written as \"HH:MM ET\" instead of \"Month DD, YYYY, HH:MM ET\"\n",
        "          date = datetime.strptime(date, '%H:%M ET') # Convert the time into a datetime variable\n",
        "          now = datetime.now(timezone('EST')).date() # Get today's date, had to add timezone because google colab operates on UTC, while prnewswire operates on EST\n",
        "          now_t = datetime.time(date) # Time the article was released\n",
        "          date = datetime.combine(now, now_t) # Date and time combined\n",
        "        \n",
        "        if (date < end_date):\n",
        "          end_date_reached = True\n",
        "          break\n",
        "        else:\n",
        "          href = \"https://www.prnewswire.com\" + anchor['href'] # Retrieving href for the article and converting it to visitable link\n",
        "          urls.append(href) # Adding to list of urls to visit\n",
        "      \n",
        "      page_num += 1\n",
        "    \n",
        "\n",
        "\n",
        "  return urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8910TR4jGPL"
      },
      "source": [
        "def _get_articles(urls):\n",
        "  \"\"\"\n",
        "  Gets and returns articles from a list of URLs\n",
        "\n",
        "  Args:\n",
        "      urls (list): A list of URLs, each corresponding to an article\n",
        "\n",
        "  Returns:\n",
        "      articles (list): A list of lists. Each list corresponds to one article\n",
        "                      and contains strings that represent the text within \n",
        "                      that article.\n",
        "\n",
        "  Detailed description:\n",
        "  We visit each url from the list of URLs provided as input. We first check for\n",
        "  a response status code of 200 to ensure the visit was successful. Then we feed\n",
        "  the HTML code we retrieve from the website assosciated to the URL into\n",
        "  BeautifulSoup for easy parsing. \n",
        "  \n",
        "  When observing the HTML codes for articles on PRNewswire, it was observed that \n",
        "  any content within the body of the article was contained within div containers \n",
        "  of class 'col-sm-10 col-sm-offset-1'. Furthermore, within these div containers, \n",
        "  all the text was stored within paragraph (p) tags.\n",
        "\n",
        "  So, for each article, we extract all the p tags within all the div tags of the\n",
        "  class 'col-sm-10 col-sm-offset-1' and add them to a list as strings. Once we\n",
        "  have succcessfully extracted all the p tags, we add this list to the 'articles' \n",
        "  list which stores all the articles.\n",
        "  \"\"\"\n",
        "  articles = []\n",
        "\n",
        "  for url in urls:\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.content)\n",
        "\n",
        "      divs = soup.find_all('div', class_ = 'col-sm-10 col-sm-offset-1') # Finds all div containers of the class that's meant for the body of the webpage\n",
        "\n",
        "      # Getting all the text out of the divs collected above\n",
        "      article = [] # For storing all the text within this article\n",
        "      for div in divs:\n",
        "        p_tags = div.find_all('p') # All the p tags in the current div, since all the text in the body of the prnewsire articles is always stored within p tags\n",
        "        for p in p_tags:\n",
        "          article.append(p.get_text()) # Getting the plain text within the p tag\n",
        "      articles.append(article)\n",
        "\n",
        "  return articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF4hy-nyjJJP"
      },
      "source": [
        "def scrape(NUM_DAYS):\n",
        "  \"\"\"\n",
        "  Finds all the articles published on PRNewswire within a given number of days \n",
        "  from the time this function is called.\n",
        "\n",
        "  Args:\n",
        "      NUM_DAYS (int): The number of days of articles to scrape, starting from\n",
        "                      the current day.\n",
        "\n",
        "  Returns:\n",
        "      articles (list): A list of lists. Each list corresponds to one article\n",
        "                      and contains strings that represent the text within \n",
        "                      that article.\n",
        "\n",
        "  Detailed description:\n",
        "  Calculates the end date and time based on the current date and time as well as\n",
        "  the number of days worth of articles to scrape. Then gets urls for all the\n",
        "  articles published within the time period. Then visits all the urls to get the\n",
        "  contents of the articles. Finally, returns the articles.\n",
        "  \"\"\"\n",
        "  \n",
        "  end_date = datetime.today() - timedelta(days=NUM_DAYS) # Date NUM_DAYS days ago\n",
        "  urls = _get_urls(end_date)\n",
        "  articles = _get_articles(urls)\n",
        "\n",
        "  return articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWrAK7oIjb7R"
      },
      "source": [
        "dataset = scrape(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HkhJBIhjTBH"
      },
      "source": [
        "## PART 2 - Track/Store/Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1Cw3odojXMe"
      },
      "source": [
        "# Each string within the list of articles is merged into one large string\n",
        "corpus = '' # All the articles in one large string\n",
        "for x in dataset:\n",
        "  corpus += ' '.join(str(e) for e in x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_lTWgbbju4X"
      },
      "source": [
        "def tickerCapture(corpus:str) -> list:\n",
        "    \"\"\"\n",
        "    Applies regex on a large string to find all matching instances of a stock\n",
        "    symbols.\n",
        "\n",
        "    Args:\n",
        "        corpus (str): String that contains exchang and ticker info\n",
        "\n",
        "    Returns:\n",
        "        result (list): [(<Exchange>, <ticker>)] list of <exchange> <ticker> data\n",
        "\n",
        "    Detailed Description:\n",
        "    The stock symbols are always presented in the articles in a certain format.\n",
        "    This function parses through a string and stores all instances that match \n",
        "    that format intro a list.\n",
        "    \"\"\"\n",
        "     # Expected formats examples: (NYSE: HMLP) and  (NYSE/LSE: CCL; NYSE:CUK)\n",
        "     # Example return [('NYSE', 'HMLP'), ('NYSE/LSE', 'CCL'), ('NYSE', 'CUK')]\n",
        "\n",
        "    regexpattern = r'\\b\\(?(?P<exchange>[A-Z\\/]+):\\s?(?P<ticker>[A-Z]+)(?:\\)|;)'\n",
        "\n",
        "    result = re.findall(regexpattern, corpus)\n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHEiTywzjxNC"
      },
      "source": [
        "tickers = tickerCapture(corpus)\n",
        "tickers = list(set(tickers)) # Removing duplicates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAGjWeOej7TQ"
      },
      "source": [
        "# PART 3 - Retrieve Data (Web (API))\n",
        "\n",
        "From the list of tickers that we retrieved, we will select the following 3 that belong to the automobile industry: TSLA, GM, and LCID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrTxqVrSkDa9"
      },
      "source": [
        "def stock_data(ticker):\n",
        "  \"\"\"\n",
        "  Gets daily historical data for stock over past 60 days. Uses yahoo finance api\n",
        "  to get the historical data.\n",
        "\n",
        "  Args:\n",
        "      ticker (str): The stock symbol\n",
        "\n",
        "  Returns:\n",
        "      resultant (pandas dataframe): The stock close price and volume over the\n",
        "                                    specified time period\n",
        "  \"\"\"\n",
        "  start_date = datetime.today() - timedelta(days=1) # Today\n",
        "  end_date = start_date - timedelta(days=60) # Day 60 days ago\n",
        "  k = get_data(ticker, start_date=start_date, end_date=end_date, index_as_date = True, interval='1d') # Getting the historical stock data via yahoo finance api\n",
        "  resultant = k[['close','volume']] # Selecting only the stock close price and volume\n",
        "  return resultant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akm5z9JVlQBX"
      },
      "source": [
        "# PART 4 - Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6KO9xvOlUDc"
      },
      "source": [
        "def generate_visualisations(ticker):\n",
        "  \"\"\"\n",
        "  Generates time-series plots for stock closing price and volume over the past\n",
        "  60 days, provided the stock symbol.\n",
        "\n",
        "  Args:\n",
        "      ticker (str): A string representing a stock symbol\n",
        "\n",
        "  Detailed Description:\n",
        "  The function first gets the data of the stock over a 60 day period. It then\n",
        "  uses plotly and matplotlib to plot the data on 2 time-series plots.\n",
        "  \"\"\"\n",
        "  # Getting info regarding the stock from the past 60 days using the ticker\n",
        "  stock_info = stock_data(ticker)\n",
        "  stock_info = stock_info.rename_axis('date').reset_index() # Info regarding stock\n",
        "\n",
        "  # Plotting time-series for volume of stock\n",
        "  fig = px.line(stock_info, x='date', y=\"volume\", title= f\"Volume of {ticker} stock\") # Plotting\n",
        "  fig.update_xaxes( # Adding the ability to conveniently narrow down or expand the window\n",
        "      rangeslider_visible=True,\n",
        "      rangeselector=dict(\n",
        "          buttons=list([\n",
        "              dict(count=7, label=\"7d\", step=\"day\", stepmode=\"backward\"),\n",
        "              dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
        "              dict(step=\"all\")\n",
        "          ])\n",
        "      )\n",
        "  )\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "  # Generating time-series for closing price of stock\n",
        "  fig = px.line(stock_info, x='date', y=\"close\", title= f\"Closing price of {ticker} stock\") # Plotting\n",
        "  fig.update_xaxes( # Adding the ability to conveniently narrow down or expand the window\n",
        "      rangeslider_visible=True,\n",
        "      rangeselector=dict(\n",
        "          buttons=list([\n",
        "              dict(count=7, label=\"7d\", step=\"day\", stepmode=\"backward\"),\n",
        "              dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
        "              dict(step=\"all\")\n",
        "          ])\n",
        "      )\n",
        "  )\n",
        "\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D05INqEflZZC"
      },
      "source": [
        "# Generating visualisations for Tesla\n",
        "generate_visualisations('TSLA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGoSgZdGla5s"
      },
      "source": [
        "# Generating visualisations for General Motors\n",
        "generate_visualisations('GM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ0RvWLSlbeO"
      },
      "source": [
        "# Generating Visualisation for Lucid Motors\n",
        "generate_visualisations('LCID')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}