{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prnewswire_scraper.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNODWWK01SFNEq9yzPoZW8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zantorym/AIDI-1100-Project/blob/main/prnewswire_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFzEOVL1I7_Z"
      },
      "source": [
        "# Importing modules\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "from datetime import timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pytz import timezone"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EY5JdFHJFI8"
      },
      "source": [
        "NUM_DAYS = 1\n",
        "end_date = datetime.today() - timedelta(days=NUM_DAYS) # Date NUM_DAYS days ago"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-qgW_ZEJlcL"
      },
      "source": [
        "# Gets urls for all the articles from start date to end date\n",
        "# Returns a list of urls\n",
        "def get_urls():\n",
        "  urls = [] # List of URLs to visit\n",
        "\n",
        "  website = \"https://www.prnewswire.com/news-releases/news-releases-list/?page=\" # Website we need to scrape from\n",
        "  page_num = 1 # Page number of the website we need to scrape from\n",
        "\n",
        "  end_date_reached = False\n",
        "\n",
        "  while not end_date_reached:\n",
        "    current_site = website + str(page_num) + \"&pagesize=100\" # Link to visit with page number, set number of articles per page to 100 so that we don't need to visit as many pages\n",
        "    response = requests.get(current_site)\n",
        "\n",
        "    if response.status_code == 200: # 200 is the standard response for a successful HTTP request\n",
        "      soup = BeautifulSoup(response.content) # Converting the plain text html code of the website into a BeautifulSoup object for easy parsing\n",
        "      anchors = soup.find_all('a', {'class': 'newsreleaseconsolidatelink display-outline', 'href': True}) # Getting all the anchors for news articles within the webpage\n",
        "\n",
        "      for anchor in anchors:\n",
        "        date = anchor.find('small').get_text()\n",
        "        try: \n",
        "          date = datetime.strptime(date, '%b %d, %Y, %H:%M ET') # Convert to datetime\n",
        "        except: # If the conversion fails, that is because the article was releasaed today and the time is written as \"HH:MM ET\" instead of \"Month DD, YYYY, HH:MM ET\"\n",
        "          date = datetime.strptime(date, '%H:%M ET') # Convert the time into a datetime variable\n",
        "          now = datetime.now(timezone('EST')).date() # Get today's date, had to add timezone because google colab operates on UTC, while prnewswire operates on EST\n",
        "          now_t = datetime.time(date) # Time the article was released\n",
        "          date = datetime.combine(now, now_t) # Date and time combined\n",
        "        \n",
        "        if (date < end_date):\n",
        "          end_date_reached = True\n",
        "          break\n",
        "        else:\n",
        "          href = \"https://www.prnewswire.com\" + anchor['href'] # Retrieving href for the article and converting it to visitable link\n",
        "          urls.append(href) # Adding to list of urls to visit\n",
        "      \n",
        "      page_num += 1\n",
        "    \n",
        "\n",
        "\n",
        "  return urls"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c09QtSezvVKI"
      },
      "source": [
        "urls = get_urls()"
      ],
      "execution_count": 105,
      "outputs": []
    }
  ]
}